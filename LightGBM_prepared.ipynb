{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import uniform as sp_randFloat\n",
    "from scipy.stats import randint as sp_randInt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "kfold = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_prepared.csv', encoding='utf-8')\n",
    "test = pd.read_csv('test_prepared.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.rename(columns={\"Pricing, Delivery_Terms_Quote_Appr\": \"Pricing_Delivery_Terms_Quote_Appr\", \"Pricing, Delivery_Terms_Approved\": \"Pricing_Delivery_Terms_Approved\"}, inplace=True)\n",
    "test.rename(columns={\"Pricing, Delivery_Terms_Quote_Appr\": \"Pricing_Delivery_Terms_Quote_Appr\", \"Pricing, Delivery_Terms_Approved\": \"Pricing_Delivery_Terms_Approved\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(columns=['Target'])\n",
    "y_train = train['Target']\n",
    "\n",
    "X_test = test.drop(columns=['Target'])\n",
    "y_test = test['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 438 tasks      | elapsed: 16.7min\n",
      "[Parallel(n_jobs=-1)]: Done 788 tasks      | elapsed: 28.4min\n",
      "[Parallel(n_jobs=-1)]: Done 810 out of 810 | elapsed: 29.4min finished\n",
      "C:\\Users\\renzo\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iteration` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_iterations is set=1000, num_iteration=1000 will be ignored. Current value: num_iterations=1000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "\n",
      "========================================================\n",
      " Results from Grid Search \n",
      "========================================================\n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " LGBMClassifier(learning_rate=0.01, max_depth=6, num_iteration=1000)\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 100, 'num_iteration': 1000}\n",
      "\n",
      " ========================================================\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  3.4min finished\n",
      "C:\\Users\\renzo\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iteration` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================\n",
      " Results from Random Search \n",
      "========================================================\n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " LGBMClassifier(learning_rate=0.3574944356443658, max_depth=7, n_estimators=345,\n",
      "               num_iteration=8964)\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.7337223530883242\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'learning_rate': 0.3574944356443658, 'max_depth': 7, 'n_estimators': 345, 'num_iteration': 8964}\n",
      "\n",
      " ========================================================\n",
      "\n",
      "\n",
      "Random Search score:  0.7337223530883242\n",
      "\n",
      "Grid Search score:  0.7649801963686393\n",
      "\n",
      "The better model found in Grid Search ... ... ... ...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def training_model(X_train, y_train):\n",
    "        model = lgb.LGBMClassifier()\n",
    "        \n",
    "        # Grid search CV\n",
    "        parameters = {'max_depth'     : [6,8,10],\n",
    "                      'learning_rate' : [0.01, 0.05, 0.1],\n",
    "                      'num_iteration' : [1000, 5000, 10000],\n",
    "                      'n_estimators'  : [100,300,500]\n",
    "                       #Add more parameters here for tuning\n",
    "                      }        \n",
    "        grid = GridSearchCV(estimator=model, param_grid = parameters, cv = kfold, \n",
    "                            verbose = 1, n_jobs = -1, refit = True)\n",
    "        grid.fit(X_train, y_train)\n",
    "\n",
    "        # Results from Grid Search\n",
    "        print(\"\\n========================================================\")\n",
    "        print(\" Results from Grid Search \" )\n",
    "        print(\"========================================================\")    \n",
    "        print(\"\\n The best estimator across ALL searched params:\\n\",\n",
    "              grid.best_estimator_)\n",
    "        print(\"\\n The best parameters across ALL searched params:\\n\",\n",
    "              grid.best_params_)\n",
    "        print(\"\\n ========================================================\")\n",
    "\n",
    "        # Random Search CV\n",
    "        parameters = {'max_depth'     : sp_randInt(6, 10),\n",
    "                      'learning_rate' : sp_randFloat(0.1, 0.9),\n",
    "                      'num_iteration' : sp_randInt(1000, 10000),\n",
    "                      'n_estimators'  : sp_randInt(100, 1000)\n",
    "                      # Add more parameters here for tuning\n",
    "                      }\n",
    "        \n",
    "        randm = RandomizedSearchCV(estimator=model, \n",
    "                                   param_distributions = parameters, cv = kfold, \n",
    "                                   n_iter = 10, verbose = 1, n_jobs = -1)\n",
    "        randm.fit(X_train, y_train)\n",
    "\n",
    "        # Results from Random Search\n",
    "        print(\"\\n========================================================\")\n",
    "        print(\" Results from Random Search \" )\n",
    "        print(\"========================================================\")    \n",
    "        print(\"\\n The best estimator across ALL searched params:\\n\",\n",
    "              randm.best_estimator_)\n",
    "        print(\"\\n The best score across ALL searched params:\\n\",\n",
    "              randm.best_score_)\n",
    "        print(\"\\n The best parameters across ALL searched params:\\n\",\n",
    "              randm.best_params_)\n",
    "        print(\"\\n ========================================================\")\n",
    "        print()\n",
    "\n",
    "        print()\n",
    "        print(\"Random Search score: \", randm.best_score_)\n",
    "        print()\n",
    "        print(\"Grid Search score: \", grid.best_score_)        \n",
    "        print()\n",
    "\n",
    "        if grid.best_score_ > randm.best_score_:\n",
    "            print(\"The better model found in Grid Search ... ... ... ...\\n\\n\")\n",
    "            return(grid.best_estimator_)\n",
    "        else:\n",
    "            print(\"The better model found in Random Search ... ... ... ...\\n\\n\")\n",
    "            return(randm.best_estimator_)\n",
    "\n",
    "model = training_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    5.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    5.2s finished\n",
      "C:\\Users\\renzo\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iteration` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation results:  [ 0.37466636  0.27179654  0.26361448 -0.14139755 -0.49731722 -0.10048725\n",
      "  0.0795181   0.19406696  0.08770016  0.05482973]\n",
      "CV Mean r2 score: 0.058699 (Std: 0.241119)\n",
      "\n",
      "========================================================\n",
      "\n",
      "{'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.01, 'max_depth': 6, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'num_iteration': 1000}\n",
      "\n",
      "========================================================\n"
     ]
    }
   ],
   "source": [
    "def cross_validatin_and_fitting(model, X_train, y_train):\n",
    "        cv_results = cross_val_score(model, X_train, y_train, cv = kfold, scoring = 'r2', \n",
    "                                 n_jobs = -1, verbose = 1)\n",
    "        # Cross Validation Results\n",
    "        print()\n",
    "        print(\"Cross Validation results: \", cv_results)\n",
    "        prt_string = \"CV Mean r2 score: %f (Std: %f)\"% (cv_results.mean(), cv_results.std())\n",
    "        print(prt_string)\n",
    "        \n",
    "        # Final fitting of the Model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        print(); print('========================================================')\n",
    "        print(); print(model.get_params(deep = True))\n",
    "        print(); print('========================================================')        \n",
    "                \n",
    "        return model\n",
    "    \n",
    "model = cross_validatin_and_fitting(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation of the trained model: \n",
      "\n",
      "R2 Score :  0.04493572695035464\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "        # Evaluate the skill of the Trained model\n",
    "        # Evaluate the skill of the Trained model\n",
    "        pred          = model.predict(X_test)\n",
    "        r2            = r2_score(y_test, pred)\n",
    "\n",
    "        \n",
    "        print(); print('Evaluation of the trained model: ')\n",
    "        print(); print('R2 Score : ', r2)\n",
    "        \n",
    "        return model\n",
    "\n",
    "model = evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5456546763120896"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(test.Target, pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm = test.loc[:, ['Opportunity_ID', 'Target']]\n",
    "subm.loc[:, ['Opportunity_ID', 'Target']].to_csv('sub_lgb_prepared.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
